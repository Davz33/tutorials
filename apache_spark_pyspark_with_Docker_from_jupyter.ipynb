{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davz33/tutorials/blob/data/apache_spark_pyspark_with_Docker_from_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5483614c-2b54-4976-97c8-8c0037bdac40",
      "metadata": {
        "id": "5483614c-2b54-4976-97c8-8c0037bdac40"
      },
      "source": [
        "# Run a stand-alone Apache Spark on a Docker Container with PySpark from a Jupyter notebook running elsewhere"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6db160c-b58e-4060-9e61-930dd14011a2",
      "metadata": {
        "id": "c6db160c-b58e-4060-9e61-930dd14011a2"
      },
      "source": [
        "> Note: I haven't tested this on G. Collab notebooks, but on my jupyter-lab local instance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0a1fdc-615a-47c3-beae-79860c262f1c",
      "metadata": {
        "id": "8f0a1fdc-615a-47c3-beae-79860c262f1c"
      },
      "source": [
        "Copyright 2023 Davide Vitiello\n",
        "> davide_vitiello@outlook.com \n",
        "\n",
        "> github.com/davz33\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfaf44c4-5160-400f-8925-51d05290404b",
      "metadata": {
        "id": "dfaf44c4-5160-400f-8925-51d05290404b",
        "outputId": "69e215eb-9d87-4460-e790-ee5c1f6c010b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.9\n",
            "pip 22.3.1 from /opt/conda/envs/tf/lib/python3.10/site-packages/pip (python 3.10)\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89397934-e39f-43eb-93a0-c07fae5cb4ee",
      "metadata": {
        "id": "89397934-e39f-43eb-93a0-c07fae5cb4ee"
      },
      "source": [
        "## Preliminary checks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066257f8-8f2e-4fa9-865e-41775b78bdd3",
      "metadata": {
        "id": "066257f8-8f2e-4fa9-865e-41775b78bdd3"
      },
      "source": [
        "Here I check I'm in the right conda env (or virtualenv, by extension):  \n",
        "I look for the presence of tensorflow, that I know to have installed , and the name of the conda-env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20de5202-ec44-4c8a-b7b0-3b125dda55bb",
      "metadata": {
        "id": "20de5202-ec44-4c8a-b7b0-3b125dda55bb",
        "outputId": "7a82c02c-46a8-4ed3-f13c-a5aa78dd02ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensorflow-estimator==2.10.0\n",
            "tensorflow-gpu @ file:///mnt/d/Davide/Dati/Download/tensorflow_gpu-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "tensorflow-io-gcs-filesystem==0.30.0\n",
            "tf\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep tensorflow\n",
        "!echo $CONDA_DEFAULT_ENV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da0484f-d066-4b7d-9649-39dea1a45679",
      "metadata": {
        "id": "2da0484f-d066-4b7d-9649-39dea1a45679"
      },
      "source": [
        "## Apache Spark with Pyspark docker container pull and run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1850764e-ac04-4f41-bbc1-20a014e3c181",
      "metadata": {
        "id": "1850764e-ac04-4f41-bbc1-20a014e3c181"
      },
      "source": [
        "I create a new `docker-compose.yml` with the following:  \n",
        "> note: I'm using .env with .autoenv, hence I tried to replace the default .env feeding step. You don't need to do that if your .env is meant to be fed to docker, or if you don't have any\n",
        "```yaml\n",
        "#run via 'docker compose up --env-file=dummy.env -d'\n",
        "version: '3'\n",
        "services:\n",
        "  pyspark:\n",
        "    env_file:\n",
        "      # avoid consuming .env (used by .autoenv), note: in compose v2 this does not work and it's necessary to run compose with the --env-file=/dev/null flag \n",
        "      - dummy.env\n",
        "    image: apache/spark-py:v3.2.3\n",
        "    volumes:\n",
        "      - ./pycode:/var/code:ro\n",
        "    entrypoint: /bin/sh\n",
        "    stdin_open: true \n",
        "    tty: true\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b9f9ad-3f78-4f11-b78a-422853ae6b87",
      "metadata": {
        "id": "c1b9f9ad-3f78-4f11-b78a-422853ae6b87",
        "outputId": "75155292-e651-4218-b446-95921fa4552e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7e45fa215276   apache/spark-py:v3.2.3   \"/bin/sh\"                11 hours ago   Up 11 hours                              apache_spark-pyspark-1\n"
          ]
        }
      ],
      "source": [
        "!docker ps | grep spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f12b29c-ca03-4166-b193-b534c71d61fc",
      "metadata": {
        "id": "9f12b29c-ca03-4166-b193-b534c71d61fc"
      },
      "outputs": [],
      "source": [
        "!docker exec --help"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a80feb-c3b7-492c-989a-b26425264752",
      "metadata": {
        "id": "69a80feb-c3b7-492c-989a-b26425264752"
      },
      "source": [
        "## Invoking pyspark within the docker container from a jupyter-notebook located elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd9d632-4a31-411e-b805-a50790847710",
      "metadata": {
        "id": "6fd9d632-4a31-411e-b805-a50790847710"
      },
      "outputs": [],
      "source": [
        "!mkdir pyspark_submits \n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653b3347-64a2-4002-90b5-cd755a24b168",
      "metadata": {
        "id": "653b3347-64a2-4002-90b5-cd755a24b168"
      },
      "outputs": [],
      "source": [
        "!echo \"print(1+1)\" > pyspark_submits/test_pyspark_submit.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150b7549-31ac-45d7-8ef2-f5669e9d4531",
      "metadata": {
        "id": "150b7549-31ac-45d7-8ef2-f5669e9d4531",
        "outputId": "8e4854dd-9400-48e0-f6e1-e3c44958cfb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "!docker exec apache_spark-pyspark-1 /opt/spark/bin/spark-submit \\\n",
        "    --conf spark.jars.ivy=/tmp/.ivy \\\n",
        "    /var/code/pyspark_submits/test_pyspark_submit.py \\\n",
        "    &> spark_res.txt #redirect output to a file \n",
        "!grep -Ev 'WARNING|WARN' spark_res.txt #skip printing warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9a3121-6208-4303-8097-143d588a5e75",
      "metadata": {
        "id": "1e9a3121-6208-4303-8097-143d588a5e75"
      },
      "source": [
        "## Set up an alias to invoke a pyspark statement with ease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c352eb-bb85-4ca6-82f7-390dea044a91",
      "metadata": {
        "id": "07c352eb-bb85-4ca6-82f7-390dea044a91",
        "outputId": "02ec764a-15ef-48bb-ffb2-d0414276c54a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['pyspark_submits']\n"
          ]
        }
      ],
      "source": [
        "SPARK_submits = !ls | grep submits\n",
        "print(SPARK_submits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2370f0a-f0ab-4464-a2cd-deb7af32d07f",
      "metadata": {
        "id": "f2370f0a-f0ab-4464-a2cd-deb7af32d07f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "def time_decorator(function):\n",
        "   def wrapper(*args, **kwargs):\n",
        "       if(kwargs.get('benchmark')):\n",
        "            res = %timeit function(*args, **kwargs)\n",
        "       else:\n",
        "            res = function(*args, **kwargs)\n",
        "       return res\n",
        "   return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42eed82b-4342-44af-aba5-2e0b9756cf0a",
      "metadata": {
        "id": "42eed82b-4342-44af-aba5-2e0b9756cf0a"
      },
      "outputs": [],
      "source": [
        "@time_decorator\n",
        "def spksub(statement, dump = False, nohup_installed = True, print_warns = False, benchmark = False) -> str:\n",
        "    now_ = str(time.time()).replace('.','_')\n",
        "    path_prefix = os.path.join(SPARK_submits[0],now_)\n",
        "    submit_job = str(path_prefix) + '.py'\n",
        "    \n",
        "    with open(submit_job,'w+') as f:\n",
        "        f.write(statement)\n",
        "    res_f = str(path_prefix) + '.out'  \n",
        "    mapped_sjob = os.path.join('/var/code/',submit_job)#is a: dir within container\n",
        "    \n",
        "    res = !docker exec apache_spark-pyspark-1 \\\n",
        "    /opt/spark/bin/spark-submit \\\n",
        "    --conf spark.jars.ivy=/tmp/.ivy \\\n",
        "    {mapped_sjob} \\\n",
        "    &> {res_f} #is a: dir within jupyer-lab host\n",
        "    \n",
        "    if not dump: \n",
        "        if nohup_installed:\n",
        "            !nohup rm {res_f} &;\n",
        "        else:\n",
        "            !rm {res_f}\n",
        "    res = ''\n",
        "    if not print_warns:        \n",
        "        res = !grep -Ev 'WARNING|WARN' {res_f}\n",
        "    else:\n",
        "        res = !cat {res_f}\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe75e99-5fec-46a7-a64d-792ec170334a",
      "metadata": {
        "id": "8fe75e99-5fec-46a7-a64d-792ec170334a",
        "outputId": "e7683ad8-d2ce-4a2a-e8d2-1dd92cd2d066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2']"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spksub('print(1+1)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafaa324-c684-4956-a106-929b260fdec1",
      "metadata": {
        "id": "fafaa324-c684-4956-a106-929b260fdec1",
        "outputId": "93c80aa4-b056-4e5c-f608-1cd932c39e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17 s ± 96.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "spksub('print(1+1)', benchmark = True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}